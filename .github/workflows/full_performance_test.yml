name: Full Performance Test
on:
  workflow_dispatch:
    inputs:
      cloud:
        description: 'Choose Cloud Provider'
        required: true
        type: choice
        default: Azure
        options:
          - Azure
          - AWS
          - GCP
      tyk_version:
        description: 'Tyk Gateway version'
        required: true
        type: string
        default: 'v5.7'
      tyk_profiler_enabled:
        description: 'Enabled Tyk Profiling'
        required: true
        type: boolean
        default: false
      simulate_node_failure:
        description: 'Simulate node failure during test'
        required: false
        type: boolean
        default: false
      node_failure_delay_minutes:
        description: 'Minutes to wait before simulating node failure'
        required: false
        type: number
        default: 10
      node_downtime_minutes:
        description: 'Minutes to keep the node down during failure simulation'
        required: false
        type: number
        default: 5
      guarantee_errors:
        description: 'Use iptables to guarantee connection errors during node failure'
        required: false
        type: boolean
        default: false
      test_duration_minutes:
        description: 'Test duration in minutes (30-360)'
        required: false
        type: number
        default: 30

env:
  provider: ${{ inputs.cloud == 'Azure' && 'aks' || (inputs.cloud == 'AWS' && 'eks' || 'gke') }}

concurrency:
  group: ${{ inputs.cloud }}

jobs:
  performance_test:
    name: "${{ inputs.cloud }} full performance run on Tyk ${{ inputs.tyk_version }}"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install wget
        if: ${{ inputs.tyk_profiler_enabled == true }}
        run: sudo apt-get update && sudo apt-get install -y wget

      - name: Configure AKS credentials
        if: ${{ inputs.cloud == 'Azure' }}
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Configure AWS credentials
        if: ${{ inputs.cloud == 'AWS' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ vars.AWS_CLUSTER_LOCATION }}

      - name: Authenticate into gcloud
        if: ${{ inputs.cloud == 'GCP' }}
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Install gcloud CLI
        if: ${{ inputs.cloud == 'GCP' }}
        uses: google-github-actions/setup-gcloud@v2.1.0

      - name: Install gcloud k8s auth component
        if: ${{ inputs.cloud == 'GCP' }}
        run: gcloud components install gke-gcloud-auth-plugin

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3.1.1
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
          terraform_version: "1.8.2"

      - name: Create Terraform Cloud descriptors
        run: |
          cp ${{ env.provider }}/terraform.cloud.tf.example ${{ env.provider }}/terraform.cloud.tf
          cp .github/${{ env.provider }}/deployments.tf deployments/terraform.cloud.tf
          cp .github/${{ env.provider }}/tests.tf tests/terraform.cloud.tf

      - name: Stand up AKS cluster
        if: ${{ inputs.cloud == 'Azure' }}
        run: |
          cd aks
          terraform init
          terraform apply \
            --var="cluster_location=${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AZURE_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AZURE_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AZURE_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to AKS cluster
        if: ${{ inputs.cloud == 'Azure' }}
        run: |
          az aks get-credentials \
            --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --name "pt-${{ vars.AZURE_CLUSTER_LOCATION }}"

          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Stand up EKS cluster
        if: ${{ inputs.cloud == 'AWS' }}
        run: |
          cd eks
          terraform init
          terraform apply \
            --var="cluster_location=${{ vars.AWS_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AWS_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AWS_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AWS_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to EKS cluster
        if: ${{ inputs.cloud == 'AWS' }}
        run: |
          aws eks --region "${{ vars.AWS_CLUSTER_LOCATION }}" update-kubeconfig --name "pt-${{ vars.AWS_CLUSTER_LOCATION }}"

          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Stand up GKE cluster
        if: ${{ inputs.cloud == 'GCP' }}
        run: |
          cd gke
          terraform init
          terraform apply \
            --var="project=${{ secrets.GCP_PROJECT }}" \
            --var="cluster_location=${{ vars.GCP_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.GCP_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.GCP_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.GCP_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to GKE cluster
        if: ${{ inputs.cloud == 'GCP' }}
        run: |
          gcloud container clusters get-credentials "pt-${{ vars.GCP_CLUSTER_LOCATION }}" \
            --zone "${{ vars.GCP_CLUSTER_LOCATION }}" \
            --project "${{ secrets.GCP_PROJECT }}"
          
          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Deploy testing resources
        run: |
          cd deployments
          terraform init
          terraform apply \
            --var="kubernetes_config_context=performance-testing" \
            --var="tyk_version=${{ inputs.tyk_version }}" \
            --var="tyk_license=${{ secrets.DASH_LICENSE }}" \
            --var="tyk_profiler_enabled=${{ inputs.tyk_profiler_enabled }}" \
            --auto-approve
          
          # Calculate stabilization wait time based on test duration
          # Minimum 5 minutes, maximum 15 minutes, scales with duration
          STABILIZATION_MINUTES=$(( ${{ inputs.test_duration_minutes }} >= 60 ? 10 : 5 ))
          if [[ ${{ inputs.test_duration_minutes }} -ge 180 ]]; then
            STABILIZATION_MINUTES=15
          fi
          echo "Waiting ${STABILIZATION_MINUTES} minutes for deployment to stabilize..."
          sleep $((STABILIZATION_MINUTES * 60))

      - name: Show Tyk Gateway logs
        run: |
          echo "=== Tyk Gateway Pod Logs (first 200 lines) ==="
          kubectl logs -n tyk $(kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway -o jsonpath='{.items[0].metadata.name}') --tail=200 || echo "Failed to get Tyk logs"

      - name: Run Tests
        run: |
          # Start node failure simulation in background if enabled
          if [[ "${{ inputs.simulate_node_failure }}" == "true" ]]; then
            echo "Node failure simulation enabled - will terminate a node after ${{ inputs.node_failure_delay_minutes }} minutes"
            
            (
              # Wait for specified delay FIRST
              echo "Waiting ${{ inputs.node_failure_delay_minutes }} minutes before simulating node failure..."
              sleep $((${{ inputs.node_failure_delay_minutes }} * 60))
              
              echo "=== Starting node failure simulation at $(date '+%H:%M:%S') ==="
              
              # Get nodes running gateway pods
              echo "Finding nodes with gateway pods..."
              GATEWAY_NODES=$(kubectl get pods -n tyk -l app.kubernetes.io/name=tyk-gateway -o jsonpath='{range .items[*]}{.spec.nodeName}{"\n"}{end}' | sort -u)
              
              if [[ -z "$GATEWAY_NODES" ]]; then
                echo "No nodes found with gateway pods, trying to find any pod with 'gateway' in name..."
                GATEWAY_NODES=$(kubectl get pods -n tyk --no-headers | grep gateway | awk '{print $1}' | head -1 | xargs -I {} kubectl get pod {} -n tyk -o jsonpath='{.spec.nodeName}')
              fi
              
              NODE_TO_TERMINATE=$(echo "$GATEWAY_NODES" | head -n 1)
              
              if [[ -z "$NODE_TO_TERMINATE" ]]; then
                echo "No gateway nodes found to terminate"
                exit 0
              fi
              
              echo "Selected node for termination: $NODE_TO_TERMINATE"
              
              # Cloud-specific node termination
              if [[ "${{ inputs.cloud }}" == "Azure" ]]; then
                # Get VMSS instance ID from node name
                VMSS_NAME=$(az vmss list --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" --query "[0].name" -o tsv)
                INSTANCE_ID=$(az vmss list-instances --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" --name "$VMSS_NAME" --query "[?osProfile.computerName=='$NODE_TO_TERMINATE'].instanceId" -o tsv)
                
                if [[ -n "$INSTANCE_ID" ]]; then
                  echo "Terminating Azure VMSS instance: $INSTANCE_ID"
                  az vmss delete-instances \
                    --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" \
                    --name "$VMSS_NAME" \
                    --instance-ids "$INSTANCE_ID" \
                    --no-wait
                  echo "Azure node termination initiated"
                else
                  echo "Could not find Azure VMSS instance for node: $NODE_TO_TERMINATE"
                fi
                
              elif [[ "${{ inputs.cloud }}" == "AWS" ]]; then
                # Get EC2 instance ID from node provider ID
                INSTANCE_ID=$(kubectl get node "$NODE_TO_TERMINATE" -o jsonpath='{.spec.providerID}' | cut -d'/' -f5)
                
                if [[ -n "$INSTANCE_ID" ]]; then
                  echo "Terminating AWS EC2 instance: $INSTANCE_ID"
                  aws ec2 terminate-instances \
                    --instance-ids "$INSTANCE_ID" \
                    --region "${{ vars.AWS_CLUSTER_LOCATION }}"
                  echo "AWS node termination initiated"
                else
                  echo "Could not find AWS instance ID for node: $NODE_TO_TERMINATE"
                fi
                
              elif [[ "${{ inputs.cloud }}" == "GCP" ]]; then
                # === NODE FAILURE (GKE, with visible node count reduction) ===
                INSTANCE_NAME="$NODE_TO_TERMINATE"
                ZONE="${{ vars.GCP_CLUSTER_LOCATION }}"
                CLUSTER_NAME="pt-${ZONE}"
                
                echo "Node details:"
                echo "  Instance: $INSTANCE_NAME"
                echo "  Zone: $ZONE"
                echo "  Cluster: $CLUSTER_NAME"
                
                # Count pods on the node before deletion
                echo "Pods running on node $INSTANCE_NAME before failure:"
                kubectl get pods --all-namespaces --field-selector spec.nodeName=$INSTANCE_NAME -o wide
                POD_COUNT=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$INSTANCE_NAME --no-headers | wc -l)
                GATEWAY_POD_COUNT=$(kubectl get pods -n tyk --field-selector spec.nodeName=$INSTANCE_NAME --selector=app=gateway-tyk-tyk-gateway --no-headers | wc -l)
                echo "Total pods on node: $POD_COUNT (Gateway pods: $GATEWAY_POD_COUNT)"
                
                # Check pod distribution across all nodes
                echo "Pod distribution before failure:"
                for node in $(kubectl get nodes -o name | cut -d/ -f2); do
                  count=$(kubectl get pods -n tyk --field-selector spec.nodeName=$node --no-headers | wc -l)
                  echo "  $node: $count pods"
                done
                
                if [[ -n "$INSTANCE_NAME" ]]; then
                  # Resolve the MIG that owns this instance
                  MIG_URL=$(gcloud compute instances describe "$INSTANCE_NAME" \
                    --zone "$ZONE" --format='get(metadata.items[created-by])')
                  MIG_NAME=$(basename "$MIG_URL")
                  MIG_SIZE=$(gcloud compute instance-groups managed describe "$MIG_NAME" \
                    --zone "$ZONE" --format='get(targetSize)')
                  
                  echo "=== NODE FAILURE at $(date '+%H:%M:%S') ==="
                  echo "Simulating hard failure of $INSTANCE_NAME in MIG $MIG_NAME (drop target size from $MIG_SIZE to $((MIG_SIZE-1)))"
                  
                  # Identify gateway pods on the soon-to-fail node and remove them fast from endpoints
                  GATEWAY_PODS_ON_NODE=$(kubectl get pods -n tyk -l app=gateway-tyk-tyk-gateway -o wide \
                    --field-selector spec.nodeName="$INSTANCE_NAME" --no-headers | awk '{print $1}')
                  POD_IPS=$(kubectl get pods -n tyk -l app=gateway-tyk-tyk-gateway -o wide \
                    --field-selector spec.nodeName="$INSTANCE_NAME" -o jsonpath='{.items[*].status.podIP}')
                  
                  if [[ -n "$GATEWAY_PODS_ON_NODE" ]]; then
                    echo "Force-deleting pods scheduled on $INSTANCE_NAME to drop endpoints immediately:"
                    echo "$GATEWAY_PODS_ON_NODE" | xargs -r -n1 -I{} \
                      kubectl delete pod -n tyk {} --force --grace-period=0 --wait=false
                  else
                    echo "No gateway pods found on $INSTANCE_NAME"
                  fi
                  
                  # Delete the instance and reduce MIG target size by 1 (visible node count goes 4 -> 3)
                  gcloud compute instance-groups managed delete-instances "$MIG_NAME" \
                    --instances="$INSTANCE_NAME" --zone="$ZONE" --quiet
                  echo "Deleted instance $INSTANCE_NAME; MIG target size reduced from $MIG_SIZE to $((MIG_SIZE-1))"
                  
                  # Immediately remove the Node object so 'kubectl get nodes' shows 3, not 3+NotReady
                  kubectl delete node "$INSTANCE_NAME" --ignore-not-found=true || true
                  
                  echo "Node loss triggered - observing brief impact; will resize MIG back to $MIG_SIZE shortly..."
                  
                  # Optional: Add iptables REJECT rules for guaranteed errors (30-60s)
                  if [[ "${{ inputs.guarantee_errors }}" == "true" ]]; then
                    echo "=== Adding iptables REJECT rules for immediate failures ==="
                    # Restrict the REJECTs strictly to IPs of pods that were on the failed node
                    for n in $(kubectl get nodes -o name | cut -d/ -f2 | grep -v "$INSTANCE_NAME"); do
                      echo "Adding REJECT rules on node $n for failed-node pod IPs: $POD_IPS"
                      kubectl debug node/$n --profile=sysadmin --image=nicolaka/netshoot -- \
                        bash -c "for ip in $POD_IPS; do iptables -I OUTPUT -d \$ip -p tcp -j REJECT --reject-with tcp-reset; done; sleep 60; for ip in $POD_IPS; do iptables -D OUTPUT -d \$ip -p tcp -j REJECT --reject-with tcp-reset; done" &
                    done
                  fi
                  
                  # Monitor for 30 seconds to capture the brief error window
                  for i in {1..6}; do
                    sleep 5
                    echo ""
                    echo "[$((i*5))s] Impact monitoring:"
                    echo "  Node count: $(kubectl get nodes --no-headers | wc -l) (was $MIG_SIZE)"
                    kubectl get nodes | grep -E "NAME|NotReady" || echo "    All nodes ready"
                    
                    echo "  Gateway endpoints ready:"
                    ENDPOINT_COUNT=$(kubectl get endpoints -n tyk gateway-tyk-svc-tyk-gateway -o json 2>/dev/null \
                      | jq -r '.subsets[0].addresses | length' 2>/dev/null || echo "0")
                    echo "    $ENDPOINT_COUNT endpoints"
                    
                    echo "  Gateway pod phases:"
                    kubectl get pods -n tyk -l app=gateway-tyk-tyk-gateway --no-headers \
                      | awk '{print $3}' | sort | uniq -c | awk '{print "    "$2": "$1}'
                    
                    # Show any pods that are not Running
                    NOT_RUNNING=$(kubectl get pods -n tyk -l app=gateway-tyk-tyk-gateway --no-headers | grep -v "Running" | wc -l)
                    if [[ $NOT_RUNNING -gt 0 ]]; then
                      echo "  Non-running gateway pods:"
                      kubectl get pods -n tyk -l app=gateway-tyk-tyk-gateway --no-headers | grep -v "Running" | awk '{print "    "$1": "$3}'
                    fi
                    
                    # Check HPA status
                    echo "  HPA status:"
                    kubectl get hpa -n tyk --no-headers | awk '{print "    "$1": current="$2"/"$3", CPU="$4}'
                  done
                  
                  echo ""
                  echo "Resizing MIG $MIG_NAME back to $MIG_SIZE..."
                  gcloud compute instance-groups managed resize "$MIG_NAME" --size="$MIG_SIZE" --zone="$ZONE" --quiet
                  echo "MIG resized back to $MIG_SIZE - new node will be provisioned"
                else
                  echo "Could not find GCP instance for node: $NODE_TO_TERMINATE"
                fi
              fi
              
              echo "=== Node failure simulation completed ==="
              
              # Show cluster status after termination
              sleep 30
              echo "=== Cluster status after node termination ==="
              kubectl get nodes
              echo "=== Gateway pods status ==="
              kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway
            ) &
            
            echo "Node failure simulation scheduled in background"
          fi
          
          # Run the actual tests
          cd tests
          terraform init
          terraform apply \
            --var="kubernetes_config_context=performance-testing" \
            --var="tests_duration=${{ inputs.test_duration_minutes }}" \
            --auto-approve

      - name: Show Tyk Gateway logs after tests
        run: |
          echo "=== Tyk Gateway Pod Logs After Tests (last 200 lines) ==="
          kubectl logs -n tyk $(kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway -o jsonpath='{.items[0].metadata.name}') --tail=200 || echo "Failed to get Tyk logs"

      - name: Test Grafana Snapshot
        continue-on-error: true
        run: |
          kubectl logs -n dependencies $(kubectl get pods -n dependencies --selector=app=snapshot-job -o jsonpath='{.items[-1].metadata.name}') --tail=1

      - name: Download Profiles
        if: ${{ inputs.tyk_profiler_enabled == true }}
        continue-on-error: true
        run: |
          kubectl port-forward svc/gateway-svc-tyk-tyk-gateway -n tyk 8080 &
          sleep 5

          mkdir profiles
          wget "http://localhost:8080/debug/pprof/heap?debug=1"       -O ./profiles/${{ inputs.tyk_version }}-heap.txt
          wget "http://localhost:8080/debug/pprof/allocs?debug=1"     -O ./profiles/${{ inputs.tyk_version }}-allocs.txt
          wget "http://localhost:8080/debug/pprof/goroutine?debug=1"  -O ./profiles/${{ inputs.tyk_version }}-goroutine.txt

      - name: Upload Profiles
        if: ${{ inputs.tyk_profiler_enabled == true }}
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.tyk_version }}-profiles
          path: ./profiles
          if-no-files-found: warn

      - name: Destroy Tests
        if: always()
        run: |
          cd tests
          terraform destroy \
            --var="kubernetes_config_context=performance-testing" \
            --auto-approve

      - name: Destroy testing resources
        if: always()
        run: |
          cd deployments
          terraform destroy \
            --var="kubernetes_config_context=performance-testing" \
            --var="tyk_license=${{ secrets.DASH_LICENSE }}" \
            --auto-approve

      - name: Destroy AKS cluster
        if: always() && inputs.cloud == 'Azure'
        run: |
          cd aks
          terraform init
          terraform destroy \
            --var="cluster_location=${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AZURE_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AZURE_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AZURE_TEST_MACHINE_TYPE }}" \
            --auto-approve

      - name: Destroy EKS cluster
        if: always() && inputs.cloud == 'AWS'
        run: |
          cd eks
          terraform init
          terraform destroy \
            --var="cluster_location=${{ vars.AWS_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AWS_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AWS_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AWS_TEST_MACHINE_TYPE }}" \
            --auto-approve

      - name: Destroy GKE cluster
        if: always() && inputs.cloud == 'GCP'
        run: |
          cd gke
          terraform init
          terraform destroy \
            --var="project=${{ secrets.GCP_PROJECT }}" \
            --var="cluster_location=${{ vars.GCP_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.GCP_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.GCP_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.GCP_TEST_MACHINE_TYPE }}" \
            --auto-approve
