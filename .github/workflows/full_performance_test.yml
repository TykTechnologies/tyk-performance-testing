name: Full Performance Test
on:
  workflow_dispatch:
    inputs:
      cloud:
        description: 'Choose Cloud Provider'
        required: true
        type: choice
        default: Azure
        options:
          - Azure
          - AWS
          - GCP
      tyk_version:
        description: 'Tyk Gateway version'
        required: true
        type: string
        default: 'v5.7'
      tyk_profiler_enabled:
        description: 'Enabled Tyk Profiling'
        required: true
        type: boolean
        default: false
      simulate_node_failure:
        description: 'Simulate node failure during test'
        required: false
        type: boolean
        default: false
      node_failure_delay_minutes:
        description: 'Minutes to wait before simulating node failure'
        required: false
        type: number
        default: 10
      test_duration_minutes:
        description: 'Test duration in minutes (30-360)'
        required: false
        type: number
        default: 30

env:
  provider: ${{ inputs.cloud == 'Azure' && 'aks' || (inputs.cloud == 'AWS' && 'eks' || 'gke') }}

concurrency:
  group: ${{ inputs.cloud }}

jobs:
  performance_test:
    name: "${{ inputs.cloud }} full performance run on Tyk ${{ inputs.tyk_version }}"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install wget
        if: ${{ inputs.tyk_profiler_enabled == true }}
        run: sudo apt-get update && sudo apt-get install -y wget

      - name: Configure AKS credentials
        if: ${{ inputs.cloud == 'Azure' }}
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Configure AWS credentials
        if: ${{ inputs.cloud == 'AWS' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ vars.AWS_CLUSTER_LOCATION }}

      - name: Authenticate into gcloud
        if: ${{ inputs.cloud == 'GCP' }}
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Install gcloud CLI
        if: ${{ inputs.cloud == 'GCP' }}
        uses: google-github-actions/setup-gcloud@v2.1.0

      - name: Install gcloud k8s auth component
        if: ${{ inputs.cloud == 'GCP' }}
        run: gcloud components install gke-gcloud-auth-plugin

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3.1.1
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
          terraform_version: "1.8.2"

      - name: Create Terraform Cloud descriptors
        run: |
          cp ${{ env.provider }}/terraform.cloud.tf.example ${{ env.provider }}/terraform.cloud.tf
          cp .github/${{ env.provider }}/deployments.tf deployments/terraform.cloud.tf
          cp .github/${{ env.provider }}/tests.tf tests/terraform.cloud.tf

      - name: Stand up AKS cluster
        if: ${{ inputs.cloud == 'Azure' }}
        run: |
          cd aks
          terraform init
          terraform apply \
            --var="cluster_location=${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AZURE_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AZURE_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AZURE_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to AKS cluster
        if: ${{ inputs.cloud == 'Azure' }}
        run: |
          az aks get-credentials \
            --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --name "pt-${{ vars.AZURE_CLUSTER_LOCATION }}"

          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Stand up EKS cluster
        if: ${{ inputs.cloud == 'AWS' }}
        run: |
          cd eks
          terraform init
          terraform apply \
            --var="cluster_location=${{ vars.AWS_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AWS_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AWS_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AWS_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to EKS cluster
        if: ${{ inputs.cloud == 'AWS' }}
        run: |
          aws eks --region "${{ vars.AWS_CLUSTER_LOCATION }}" update-kubeconfig --name "pt-${{ vars.AWS_CLUSTER_LOCATION }}"

          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Stand up GKE cluster
        if: ${{ inputs.cloud == 'GCP' }}
        run: |
          cd gke
          terraform init
          terraform apply \
            --var="project=${{ secrets.GCP_PROJECT }}" \
            --var="cluster_location=${{ vars.GCP_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.GCP_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.GCP_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.GCP_TEST_MACHINE_TYPE }}" \
            --var="services_nodes_count=4" \
            --auto-approve

      - name: Connect to GKE cluster
        if: ${{ inputs.cloud == 'GCP' }}
        run: |
          gcloud container clusters get-credentials "pt-${{ vars.GCP_CLUSTER_LOCATION }}" \
            --zone "${{ vars.GCP_CLUSTER_LOCATION }}" \
            --project "${{ secrets.GCP_PROJECT }}"
          
          kubectl config rename-context $(kubectl config current-context) performance-testing

      - name: Deploy testing resources
        run: |
          cd deployments
          terraform init
          terraform apply \
            --var="kubernetes_config_context=performance-testing" \
            --var="tyk_version=${{ inputs.tyk_version }}" \
            --var="tyk_license=${{ secrets.DASH_LICENSE }}" \
            --var="tyk_profiler_enabled=${{ inputs.tyk_profiler_enabled }}" \
            --auto-approve
          
          # Calculate stabilization wait time based on test duration
          # Minimum 5 minutes, maximum 15 minutes, scales with duration
          STABILIZATION_MINUTES=$(( ${{ inputs.test_duration_minutes }} >= 60 ? 10 : 5 ))
          if [[ ${{ inputs.test_duration_minutes }} -ge 180 ]]; then
            STABILIZATION_MINUTES=15
          fi
          echo "Waiting ${STABILIZATION_MINUTES} minutes for deployment to stabilize..."
          sleep $((STABILIZATION_MINUTES * 60))

      - name: Show Tyk Gateway logs
        run: |
          echo "=== Tyk Gateway Pod Logs (first 200 lines) ==="
          kubectl logs -n tyk $(kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway -o jsonpath='{.items[0].metadata.name}') --tail=200 || echo "Failed to get Tyk logs"

      - name: Run Tests
        run: |
          # Start node failure simulation in background if enabled
          if [[ "${{ inputs.simulate_node_failure }}" == "true" ]]; then
            echo "Node failure simulation enabled - will terminate a node after ${{ inputs.node_failure_delay_minutes }} minutes"
            
            (
              # Wait for specified delay
              sleep $((${{ inputs.node_failure_delay_minutes }} * 60))
              
              echo "=== Starting node failure simulation ==="
              
              # Get nodes running gateway pods
              GATEWAY_NODES=$(kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway -o jsonpath='{range .items[*]}{.spec.nodeName}{"\n"}{end}' | sort -u)
              NODE_TO_TERMINATE=$(echo "$GATEWAY_NODES" | head -n 1)
              
              if [[ -z "$NODE_TO_TERMINATE" ]]; then
                echo "No gateway nodes found to terminate"
                exit 0
              fi
              
              echo "Selected node for termination: $NODE_TO_TERMINATE"
              
              # Cloud-specific node termination
              if [[ "${{ inputs.cloud }}" == "Azure" ]]; then
                # Get VMSS instance ID from node name
                VMSS_NAME=$(az vmss list --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" --query "[0].name" -o tsv)
                INSTANCE_ID=$(az vmss list-instances --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" --name "$VMSS_NAME" --query "[?osProfile.computerName=='$NODE_TO_TERMINATE'].instanceId" -o tsv)
                
                if [[ -n "$INSTANCE_ID" ]]; then
                  echo "Terminating Azure VMSS instance: $INSTANCE_ID"
                  az vmss delete-instances \
                    --resource-group "pt-${{ vars.AZURE_CLUSTER_LOCATION }}" \
                    --name "$VMSS_NAME" \
                    --instance-ids "$INSTANCE_ID" \
                    --no-wait
                  echo "Azure node termination initiated"
                else
                  echo "Could not find Azure VMSS instance for node: $NODE_TO_TERMINATE"
                fi
                
              elif [[ "${{ inputs.cloud }}" == "AWS" ]]; then
                # Get EC2 instance ID from node provider ID
                INSTANCE_ID=$(kubectl get node "$NODE_TO_TERMINATE" -o jsonpath='{.spec.providerID}' | cut -d'/' -f5)
                
                if [[ -n "$INSTANCE_ID" ]]; then
                  echo "Terminating AWS EC2 instance: $INSTANCE_ID"
                  aws ec2 terminate-instances \
                    --instance-ids "$INSTANCE_ID" \
                    --region "${{ vars.AWS_CLUSTER_LOCATION }}"
                  echo "AWS node termination initiated"
                else
                  echo "Could not find AWS instance ID for node: $NODE_TO_TERMINATE"
                fi
                
              elif [[ "${{ inputs.cloud }}" == "GCP" ]]; then
                # Get instance name and zone from node
                INSTANCE_NAME="$NODE_TO_TERMINATE"
                ZONE="${{ vars.GCP_CLUSTER_LOCATION }}"
                CLUSTER_NAME="pt-${ZONE}"
                
                # Get the node pool name from the node labels
                NODE_POOL=$(kubectl get node "$NODE_TO_TERMINATE" -o jsonpath='{.metadata.labels.cloud\.google\.com/gke-nodepool}')
                
                echo "Node details:"
                echo "  Instance: $INSTANCE_NAME"
                echo "  Zone: $ZONE"
                echo "  Node Pool: $NODE_POOL"
                echo "  Cluster: $CLUSTER_NAME"
                
                # First, disable auto-repair for the node pool to prevent automatic recovery
                echo "Disabling auto-repair for node pool: $NODE_POOL"
                gcloud container node-pools update "$NODE_POOL" \
                  --cluster="$CLUSTER_NAME" \
                  --zone="$ZONE" \
                  --no-enable-autorepair \
                  --quiet || echo "Warning: Failed to disable auto-repair"
                
                # Count pods on the node before deletion
                echo "Pods running on node $INSTANCE_NAME:"
                kubectl get pods --all-namespaces --field-selector spec.nodeName=$INSTANCE_NAME -o wide
                POD_COUNT=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$INSTANCE_NAME --no-headers | wc -l)
                GATEWAY_POD_COUNT=$(kubectl get pods -n tyk --field-selector spec.nodeName=$INSTANCE_NAME --selector=app=gateway-tyk-tyk-gateway --no-headers | wc -l)
                echo "Total pods on node: $POD_COUNT (Gateway pods: $GATEWAY_POD_COUNT)"
                
                # Check pod distribution across all nodes
                echo "Pod distribution before deletion:"
                for node in $(kubectl get nodes -o name | cut -d/ -f2); do
                  count=$(kubectl get pods -n tyk --field-selector spec.nodeName=$node --no-headers | wc -l)
                  echo "  $node: $count pods"
                done
                
                if [[ -n "$INSTANCE_NAME" ]]; then
                  # Get current node pool size before deletion
                  CURRENT_SIZE=$(gcloud container node-pools describe "$NODE_POOL" \
                    --cluster="$CLUSTER_NAME" \
                    --zone="$ZONE" \
                    --format="value(initialNodeCount)" 2>/dev/null || echo "4")
                  echo "Current node pool size: $CURRENT_SIZE"
                  
                  # OPTIONAL: Cordon other nodes to prevent pod scheduling (more chaos)
                  echo "=== CORDONING other nodes to prevent rescheduling ==="
                  for node in $(kubectl get nodes -o name | cut -d/ -f2 | grep -v "$INSTANCE_NAME"); do
                    echo "Cordoning node: $node"
                    kubectl cordon $node
                  done
                  
                  # STEP 1: Force delete the instance immediately (causes abrupt failure)
                  echo "=== FORCE DELETING GCP instance: $INSTANCE_NAME ==="
                  gcloud compute instances delete "$INSTANCE_NAME" \
                    --zone "$ZONE" \
                    --quiet &
                  DELETE_PID=$!
                  
                  echo "Force deletion initiated, waiting for completion..."
                  wait $DELETE_PID
                  echo "Instance $INSTANCE_NAME has been forcefully deleted!"
                  
                  # Monitor immediate impact
                  echo "Monitoring immediate impact for 60 seconds..."
                  for i in {1..12}; do
                    sleep 5
                    echo "[$((i*5))s] Cluster state after deletion:"
                    echo "  Nodes: $(kubectl get nodes --no-headers | wc -l) (was $CURRENT_SIZE)"
                    kubectl get nodes | grep -E "NAME|NotReady" || true
                    
                    echo "  Pod states in tyk namespace:"
                    kubectl get pods -n tyk --no-headers | awk '{print $3}' | sort | uniq -c
                    
                    echo "  Endpoints for gateway service:"
                    kubectl get endpoints -n tyk gateway-tyk-svc-tyk-gateway -o jsonpath='{.subsets[*].addresses[*].ip}' | wc -w
                    
                    echo "  Pods NOT ready:"
                    kubectl get pods -n tyk --no-headers | grep -v "1/1" | wc -l
                    
                    echo "  Service endpoint details:"
                    kubectl get endpoints -n tyk gateway-tyk-svc-tyk-gateway -o yaml | grep -A2 "notReadyAddresses:" || echo "    No not-ready addresses"
                  done
                  
                  # Uncordon nodes to allow scheduling again
                  echo "=== UNCORDONING nodes to allow rescheduling ==="
                  for node in $(kubectl get nodes -o name | cut -d/ -f2); do
                    kubectl uncordon $node || true
                  done
                  
                  # STEP 2: Now resize the node pool to restore capacity
                  # This ensures GKE creates a new node instead of just auto-healing
                  echo "=== RESIZING node pool to restore capacity ==="
                  NEW_SIZE=$((CURRENT_SIZE - 1))
                  echo "Resizing node pool from $CURRENT_SIZE to $NEW_SIZE (temporary)..."
                  gcloud container clusters resize "$CLUSTER_NAME" \
                    --node-pool="$NODE_POOL" \
                    --num-nodes="$NEW_SIZE" \
                    --zone="$ZONE" \
                    --quiet
                  
                  sleep 10
                  
                  echo "Resizing back to original size $CURRENT_SIZE..."
                  gcloud container clusters resize "$CLUSTER_NAME" \
                    --node-pool="$NODE_POOL" \
                    --num-nodes="$CURRENT_SIZE" \
                    --zone="$ZONE" \
                    --quiet
                  
                  echo "Resize initiated. Monitoring recovery for 30 seconds..."
                  for i in {1..6}; do
                    sleep 5
                    echo "[$((i*5))s] Recovery state:"
                    echo "  Nodes: $(kubectl get nodes --no-headers | wc -l) / $CURRENT_SIZE"
                    kubectl get pods -n tyk | grep -vE "Running|Completed" | wc -l
                  done
                else
                  echo "Could not find GCP instance for node: $NODE_TO_TERMINATE"
                fi
              fi
              
              echo "=== Node failure simulation completed ==="
              
              # Show cluster status after termination
              sleep 30
              echo "=== Cluster status after node termination ==="
              kubectl get nodes
              echo "=== Gateway pods status ==="
              kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway
            ) &
            
            echo "Node failure simulation scheduled in background"
          fi
          
          # Run the actual tests
          cd tests
          terraform init
          terraform apply \
            --var="kubernetes_config_context=performance-testing" \
            --var="tests_duration=${{ inputs.test_duration_minutes }}" \
            --auto-approve

      - name: Show Tyk Gateway logs after tests
        run: |
          echo "=== Tyk Gateway Pod Logs After Tests (last 200 lines) ==="
          kubectl logs -n tyk $(kubectl get pods -n tyk --selector=app=gateway-tyk-tyk-gateway -o jsonpath='{.items[0].metadata.name}') --tail=200 || echo "Failed to get Tyk logs"

      - name: Test Grafana Snapshot
        continue-on-error: true
        run: |
          kubectl logs -n dependencies $(kubectl get pods -n dependencies --selector=app=snapshot-job -o jsonpath='{.items[-1].metadata.name}') --tail=1

      - name: Download Profiles
        if: ${{ inputs.tyk_profiler_enabled == true }}
        continue-on-error: true
        run: |
          kubectl port-forward svc/gateway-svc-tyk-tyk-gateway -n tyk 8080 &
          sleep 5

          mkdir profiles
          wget "http://localhost:8080/debug/pprof/heap?debug=1"       -O ./profiles/${{ inputs.tyk_version }}-heap.txt
          wget "http://localhost:8080/debug/pprof/allocs?debug=1"     -O ./profiles/${{ inputs.tyk_version }}-allocs.txt
          wget "http://localhost:8080/debug/pprof/goroutine?debug=1"  -O ./profiles/${{ inputs.tyk_version }}-goroutine.txt

      - name: Upload Profiles
        if: ${{ inputs.tyk_profiler_enabled == true }}
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.tyk_version }}-profiles
          path: ./profiles
          if-no-files-found: warn

      - name: Destroy Tests
        if: always()
        run: |
          cd tests
          terraform destroy \
            --var="kubernetes_config_context=performance-testing" \
            --auto-approve

      - name: Destroy testing resources
        if: always()
        run: |
          cd deployments
          terraform destroy \
            --var="kubernetes_config_context=performance-testing" \
            --var="tyk_license=${{ secrets.DASH_LICENSE }}" \
            --auto-approve

      - name: Destroy AKS cluster
        if: always() && inputs.cloud == 'Azure'
        run: |
          cd aks
          terraform init
          terraform destroy \
            --var="cluster_location=${{ vars.AZURE_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AZURE_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AZURE_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AZURE_TEST_MACHINE_TYPE }}" \
            --auto-approve

      - name: Destroy EKS cluster
        if: always() && inputs.cloud == 'AWS'
        run: |
          cd eks
          terraform init
          terraform destroy \
            --var="cluster_location=${{ vars.AWS_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.AWS_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.AWS_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.AWS_TEST_MACHINE_TYPE }}" \
            --auto-approve

      - name: Destroy GKE cluster
        if: always() && inputs.cloud == 'GCP'
        run: |
          cd gke
          terraform init
          terraform destroy \
            --var="project=${{ secrets.GCP_PROJECT }}" \
            --var="cluster_location=${{ vars.GCP_CLUSTER_LOCATION }}" \
            --var="cluster_machine_type=${{ vars.GCP_CLUSTER_MACHINE_TYPE }}" \
            --var="upstream_machine_type=${{ vars.GCP_UPSTREAM_MACHINE_TYPE }}" \
            --var="tests_machine_type=${{ vars.GCP_TEST_MACHINE_TYPE }}" \
            --auto-approve
